package dmdn2.ir;
import org.apache.commons.lang3.StringUtils;

public class Tokenaizer {
	
	public static String clean(String x) {
		          
		//String x= "\"x\\r\\n*\\r\\nsin\\r\\n+\\r\\n*\\r\\nsin\\r\\n+\\r\\nneuron\\r\\nneuronèàòèèàòèòèàòèàòèàèò'\\r\\nneuron\\r\\nneuron neuron neuron\\r\\nneuron neuron\\r\\nneuron\\r\\n   set\\r\\nelement \\r\\n...\\r\\ninputs\\r\\noutput\\r\\n*\\r\\nb\\r\\n−\\r\\nelement \\r\\n   set\\r\\noutput\\r\\ninputs\\r\\na\\r\\nFigure 2: Examples of functions represented by a graph of computations, where each node is taken in\\r\\nsome set of allowed computations. Left: the elements are {∗, +, sin} ∪\\r\\n\\u0000\\r\\n. The architecture computes\\r\\nx∗sin(a∗x+b) and has depth 4. Right: the elements are artificial neurons computing f(x) = tanh(b+w′x);\\r\\neach element in the set has a different (w, b) parameter. The architecture is a multi-layer neural network of\\r\\ndepth 3.\\r\\nthe outputs of other nodes of the graph. For example, in a logic circuit each node can compute a boolean\\r\\nfunction taken from a small set of boolean functions. The graph as a whole has input nodes and output nodes\\r\\nand computes a function from input to output. The depth of an architecture is the maximum length of a path\\r\\nfrom any input of the graph to any output of the graph, i.e. 3 in the case of x ∗ sin(a ∗ x + b) in Figure 2.\\r\\n• If we include affine operations and sigmoids in the set of computational elements, linear regression\\r\\nand logistic regression have depth 1, i.e., have a single level.\\r\\n• When we put a fixed kernel computation K(u, v) in the set of allowed operations, along with affine\\r\\noperations, kernel machines (Schölkopf, Burges, & Smola, 1999a) with a fixed kernel can be consid-\\r\\nered to have two levels. The first level has one element computing K(x, xi) for each prototype xi (a\\r\\nselected representative training example) and matches the input vector x with the prototypes xi. The\\r\\nsecond level performs a linear combination\\r\\n∑\\r\\ni αiK(x, xi) to associate the matching prototypes xi\\r\\nwith the expected response.\\r\\n• When we put artificial neurons (affine transformation followed by a non-linearity) in our set of el-\\r\\nements, we obtain ordinary multi-layer neural networks (Rumelhart et al., 1986a). With the most\\r\\ncommon choice of one hidden layer, they also have depth two (the hidden layer and the output layer).\\r\\n• Decision trees can also be seen as having two levels, as discussed in Section 3.3.\\r\\n• Boosting (Freund & Schapire, 1996) usually adds one level to its base learners: that level computes a\\r\\nvote or linear combination of the outputs of the base learners.\\r\\n• Stacking (Wolpert, 1992) is another meta-learning algorithm that adds one level.\\r\\n• Based on current knowledge of brain anatomy (Serre, Kreiman, Kouh, Cadieu, Knoblich, & Poggio,\\r\\n2007), it appears that the cortex can be seen as a deep architecture, e.g., consider the many so-called\\r\\nlayers in the visual system.\\r\\nAlthough depth depends on the choice of the set of allowed computations for each element, theoretical\\r\\nresults suggest that it is not the absolute number of levels that matters, but the number of levels relative to\\r\\nhow many are required to represent efficiently the target function (with some choice of set of computational\\r\\nelements). As we will describe, if a function can be compactly represented with k levels using a particular\\r\\n6\\r\\n";
		//x=x+"1.1 Desiderata for Learning AI\\r\\nSummarizing some of the above issues, we state a number of requirements we perceive for learning algo-\\r\\nrithms to solve AI.\\r\\n• Ability to learn complex, highly-varying functions, i.e., with a number of variations much greater than\\r\\nthe number of training examples.\\r\\n• Ability to learn with little human input the low-level, intermediate, and high-level abstractions that\\r\\nwould be useful to represent the kind of complex functions needed for AI tasks.\\r\\n• Ability to learn from a very large set of examples: computation time for training should scale well\\r\\nwith the number of examples, i.e. close to linearly.\\r\\n• Ability to learn from mostly unlabeled data, i.e. to work in the semi-supervised setting, where not all\\r\\nthe examples come with the “right” associated labels.\\r\\n• Ability to exploit the synergies present across a large number of tasks, i.e. multi-task learning. These\\r\\nsynergies exist because all the AI tasks provide different views on the same underlying reality.\\r\\n• In the limit of a large number of tasks and when future tasks are not known ahead of time, strong\\r\\nunsupervised learning (i.e. capturing the statistical structure in the observed data) is an important\\r\\nelement of the solution.\\r\\nOther elements are equally important but are not directly connected to the material in this paper. They\\r\\ninclude the ability to learn to represent context of varying length and structure (Pollack, 1990), so as to\\r\\nallow machines to operate in a stream of observations and produce a stream of actions, the ability to make\\r\\ndecisions when actions influence the future observations and future rewards (Sutton & Barto, 1998), and the\\r\\nability to influence future observations so as to collect more relevant information about the world (i.e. a form\\r\\nof active learning (Cohn, Ghahramani, & Jordan, 1995)).\\r\\n2 Theoretical Limitations of Shallow Architectures\\r\\nIn this section, we present an argument in favor of deep architecture models by way of theoretical results re-\\r\\nvealing limitations of archictectures with insufficient depth. This part of the paper (this section and the next)\\r\\nmotivate the algorithms described in the later sections, and can be skipped without making the remainder\\r\\ndifficult to follow. The main conclusion of this section is that functions that can be compactly represented\\r\\nby a depth k architecture might require an exponential number of computational elements to be represented\\r\\nby a depth k − 1 architecture. Since the number of computational elements one can afford depends on the\\r\\nnumber of training examples available to tune or select them, the consequences are not just computational\\r\\nbut also statistical: poor generalization may be expected when using an insufficiently deep architecture for\\r\\nrepresenting some functions.\\r\\nWe consider the case of fixed-dimension inputs, where the computation performed by the machine can be\\r\\nrepresented by a directed acyclic graph where each node performs a computation that is the application of\\r\\na function on its inputs, each of which is the output of another node in the graph or one of the external\\r\\ninputs to the graph. The whole graph can be viewed as a circuit that computes a function applied to the\\r\\nexternal inputs. When the set of functions allowed for the computation nodes is limited to logic gates, such\\r\\nas { AND, OR, NOT }, this is a boolean circuit, or logic circuit.\\r\\nLet us return to the notion of depth with more examples of architectures of different depths. Consider the\\r\\nfunction f(x) = x ∗ sin(a ∗ x + b). It can be expressed as the composition of simple operations such as\\r\\naddition, subtraction, multiplication, and the sin operation, as illustrated in Figure 2. In the example, there\\r\\nwould be a different node for the multiplication a ∗ x and for the final multiplication by x. Each node in\\r\\nthe graph is associated with an output value obtained by applying some function on input values that are\\r\\n5\\r\\n";
		//x=x+"5.1. MACCHINE DI TURING A NASTRO SINGOLO 177\\r\\nIn effetti, per risolvere tale problema sarebbe necessario avere a disposi-\\r\\nzione una diversa macchina di Turing M′ la quale sia in grado di determinare\\r\\nin un tempo finito, date M e x, se la computazione eseguita da M su input\\r\\nx termina o meno. Come vedremo più avanti, tale problema, detto problema\\r\\ndella terminazione, non è risolubile, nel senso che non esiste alcuna macchina\\r\\ndi Turing con questa capacità.\\r\\nVisto che una computazione di una macchina di Turing può terminare op-\\r\\npure no, appare necessario fornire alcune precisazioni che chiariscano l’ambito\\r\\ne la modalità con cui un linguaggio può essere riconosciuto. Per prima cosa\\r\\noccorre distinguere fra riconoscimento e accettazione di un linguaggio da parte\\r\\ndi una macchina di Turing.\\r\\nDefinizione 5.6 Sia M = 〈Γ, b̄, Q, q0, F, δ〉 una macchina di Turing determi-\\r\\nnistica. Diciamo che M riconosce (decide) un linguaggio L ∈ Σ∗ (dove Σ ⊆ Γ)\\r\\nse e solo se per ogni x ∈ Σ∗ esiste una computazione massimale q0x\\r\\n∗\\r\\nM\\r\\nwqz,\\r\\ncon w ∈ ΓΓ̄∗ ∪ {ε}, z ∈ Γ̄∗Γ ∪ {b̄}, e dove q ∈ F se e solo se x ∈ L.\\r\\nLa definizione precedente non esprime alcuna condizione per il caso x 6∈ Σ∗:\\r\\nè facile però osservare che potrebbe essere estesa introdotta una macchina di\\r\\nTuring M′ che verifichi inizialmente se la condizione x ∈ Σ∗ è vera, ed operi\\r\\npoi nel modo seguente:\\r\\n• se x ∈ Σ∗, si comporta come M;\\r\\n• altrimenti, se individua in x l’occorrenza di un qualche carattere in Γ−Σ,\\r\\ntermina la propria computazione in un qualche stato non finale.\\r\\nSi noti che data una macchina di Turing deterministica M non è detto che\\r\\nesista un linguaggio deciso da M: infatti ciò è possibile se e solo se M si ferma\\r\\nper qualunque input x.\\r\\nDefinizione 5.7 Sia M = 〈Γ, b̄, Q, q0, F, δ〉 una macchina di Turing determi-\\r\\nnistica. Diciamo che M accetta un linguaggio L ∈ Σ∗ (dove Σ ⊆ Γ) se e solo\\r\\nse L = {x ∈ Σ∗ | q0x\\r\\n∗\\r\\nM\\r\\nwqz}, con w ∈ ΓΓ̄∗ ∪ {ε}, z ∈ Γ̄∗Γ ∪ {b̄}, e q ∈ F .\\r\\nEsercizio 5.4\\r\\ni) Definire una macchina di Turing deterministica che riconosce il linguaggio L =\\r\\n{ww̃ | w ∈ {a, b}+}.\\r\\nii) Definire una macchina di Turing deterministica che accetta il linguaggio L\\r\\nsopra definito e che per qualche stringa x ∈ {a, b}∗ −L cicla indefinitamente.\\r\\nVale la pena segnalare che le convenzioni poste nell’ambito dei problemi\\r\\ndi riconoscimento di linguaggi non sono le uniche possibili.\\r\\n";
		
		x=x.replace("\\n", " ").replace("\\r", " ").replace("\r"," ").replace("\\n", " ");
		x=StringUtils.stripAccents(x);
		x=x.replaceAll("[^a-zA-Z ]", "");
		x=StringUtils.lowerCase( StringUtils.normalizeSpace(x));
		return x;
	}
}
